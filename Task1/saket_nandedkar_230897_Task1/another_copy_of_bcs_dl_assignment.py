# -*- coding: utf-8 -*-
"""Another copy of BCS_DL_assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XhFXVp9xOA01U8ZWSM613QOkd_wQ_fS6

## **Linear Regression**
We will use Linear regression for predicting house prices

We are using a Kaggle dataset- https://www.kaggle.com/harlfoxem/housesalesprediction
"""

# Lets import required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

"""### **Dataset Preparation**"""

# Execute this cell for loading dataset in a pandas dataframe

from IPython.display import clear_output
!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=16x6-8Znn2T50zFwVvKlzsdN7Jd1hpjct' -O Linear_regression_dataset

data_df = pd.read_csv("Linear_regression_dataset")

# Lets have a quick Look at dataset

print("(No of rows, No of Columns) = ",data_df.shape)
data_df.head()

"""So there are **19** features (of course we will not use id as feature :) ), and 1 variable to predict(price)

But note that the **date** column contain strings so first we will remove T00.. part from it and than convert it to numpy array.
"""

data_df['date'] = data_df['date'].str.replace('T000000', '')                                         # Remove T000000 part from data column. Hint: search about .str.replace() method. :)
data_df['date']=data_df['date'].astype(int)

data_array = (data_df.drop(columns=['id'])).to_numpy()                                            # Create a numpy array which does not have "id" field
assert (data_array.shape == (21613,20))

data_df.head()

"""Now the next task is **normalization**.

We will scale each column of dataset by x -> (x-u)/s

where u is mean(x), and s is standard deviation of u
"""

mean = np.mean(data_array, axis=0)                                 # this should be an array, each entry should be mean of a column
sd = np.std(data_array, axis=0)                                # this should be an array, each entry should be standard deviation of a column

data_array_norm = (data_array - mean)/sd

print(data_array_norm.shape)

"""The last step is to make train and test dataset and to create seperate vector for price"""

labels = data_array[:, 3]                                                                                                            # extract the price column from data

x_array_norm = np.delete(data_array_norm, 3, axis=1)                                                                                                      # delete the price column from data_array_norm. Hint: use np.delete()

x_train, x_test, y_train, y_test = train_test_split(x_array_norm,labels,test_size=0.15,random_state=42,shuffle=True)    # splitting data into test and train set.

print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)

"""### **Loss and gradient descent**
We will use mean squared error(MSE) as loss

Use the gradient descent algorithm which you learned from tutorials

Your task is to complete the following functions
"""

def loss(y_pred,y_true):
  """
  input:
  y_pred = [array] predicted value of y
  y_true = [array] ground truth

  output:
  mse: [scalar] the MES loss
  """
  mse = np.mean((y_true - y_pred)**2)                      # fill code here

  return mse

def y(x,a,b):
  """
  This function should return predicted value of y = ax+b
  input:
  x: [array] the feature vector of shape (m,n)
  a: [array] weights of shape (n,)
  b: [scalar] bias

  output:
  y_pred: [array] predicted value of y of shape (m,)
  """

  m,n = x.shape
  y_pred = np.dot(x, a) + b                   # fill code here

  assert(y_pred.shape == (m,))
  return y_pred

def gradient(x,a,b,y_true):
  """
  This function shoud return gradient of loss
  input:
  x: [array] the feature vector of shape (m,n)
  a: [array] weights of shape (n,)
  b: [scalar] bias
  y_true: [array] ground truth of shape (m,)

  output:
  grad: [tuple] a tuple (derivative with respect to a[array of shape(n,)], derivative with respect to b[scalar])
  """
  m,n = x.shape
  yp = y(x,a,b)

  da = -2 * np.dot(x.T, (y_true - yp)) / m              # write code to calculate derivative of loss with respect to a
  db = -2 * np.mean(y_true - yp)              # write code to calculate derivative of loss with respect to b

  assert(da.shape ==(n,))
  return (da,db)

def gradient_descent(x,y_true,learning_rate=0.01,epochs = 10):
  """
  This function perfroms gradient descent and minimizes loss
  input:
  x: [array] the feature vector of shape (m,n)
  y_true: [array] ground truth of shape (m,)

  output:
  loss: [array] of size (epochs,)
  weights: [tuple] (a,b)
  """
  m,n = x.shape
  loss_mse = []                                 # initialize empty list to store loss
  a = np.zeros(n)                                       # initialize a- weights and b- bias
  b = 0

  for i in range(epochs):
    # calculate derivative using gradient() function
    grad_a, grad_b = gradient(x, a, b, y_true)
    # apply gradient descent now to update a and b
    a -= learning_rate * grad_a
    b -= learning_rate * grad_b

    y_pred = y(x,a,b)
    l_mse = loss(y_true, y_pred)                                # calculate loss at this point
    loss_mse.append(l_mse)

    print("Epoch ",i+1," Completed!","loss = ",l_mse)

  print("Training completed!!")

  assert(a.shape==(n,))

  return (loss_mse,a,b)

"""### **Training**"""

epochs = 10             # tweak this!!!
learn_rate = 0.03          # choose learning rate wisely otherwise loss may diverge!!

train_loss,a,b = gradient_descent(x_train ,y_train ,learn_rate,epochs )

"""### **Evaluation and Visualization**
Lets plot how loss varies with epochs

"""

#test_loss = loss(np.random.rand(len(y_test)), y_test)

#print("Loss on test data = ",test_loss)

# Visualization of loss

plt.plot(train_loss,range(1, epochs + 1))                  # plot loss versus epochs
plt.title("Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

"""## **Deep Learning**
In this section We will build a simple multilayer perceptron network(**MLP**) in TensorFlow
"""

# Lets import the required libraries
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

"""### **Load Dataset**
We will be using MNIST dataset of handwritten digits

Just run the cell below to load dataset
"""

mnist = keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
print("No. of training examples = ",x_train.shape[0])
print("Size of each image in dataset = ",x_train.shape[1:])
print("No. of test examples = ",x_test.shape[0])

# Run this cell to visualize some of the images from dataset

n = 5    # = no. of images to visualize

index = np.random.choice(x_train.shape[0],5)  # choose random index
print("label: ",end="")

for i,ind in enumerate(index):
    plt.subplot(1,n,i+1)
    plt.imshow(x_train[ind],cmap="gray")
    plt.axis("off")
    print(y_train[ind],end="       ")

plt.show()

"""#### Preprocess dataset
Since we are building a MLP model the input to the model should be a vector rather than a 28 by 28 matrix.

So your **First Task** is to flatten the images

(Hint: use *reshape()* method of arrays...)

Next, create validation dataset out of training dataset.

You can use 50K images for training and 10K for validation
"""

# Flatten the images into 1-d vectors

x_train_flatten = x_train.reshape(x_train.shape[0], -1)                                       # flatten the images of training set
x_test_flatten = x_test.reshape(x_test.shape[0], -1)                                        # flatten th eimages of test set


# Divide the training data into training and validation data....

n_validation = 10000                                        # choose number of images to be used for validation

x_validation = x_train_flatten[:n_validation]
y_validation = y_train[:n_validation]

x_train_flatten = x_train_flatten[n_validation:]
y_train = y_train[n_validation:]

"""### **Build a model**
You can choose whatever architechure you want, but ensure that it is **not too deep** as that will take too much time to train and **not too shallow** as that will give very low accuracy.
"""

model = keras.models.Sequential([
     #keras.layers(input_shape=( 28, 28)),
     keras.layers.Dense(128, activation='relu' , input_shape=(784,)),
     keras.layers.Dense(64, activation='relu'),
     keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

# Make a graphical representation of the model...
keras.utils.plot_model(model,show_shapes=True)
model.summary()

"""#### Compile and Train
Choose an optimizer- method that minimizes loss function

**adam** optimizer is one of the popular choices. You should read about these online
"""

model.compile(optimizer="adam",loss = "sparse_categorical_crossentropy",metrics=["accuracy"])
print(x_train_flatten.shape)
#n_epochs = 10           # set number of epochs
#batch_size =600           # you can tweak with these parametrs
history = model.fit(x_train_flatten, y_train, epochs=9, batch_size=500, validation_data=(x_validation, y_validation))

"""### **Evaluate**
Evaluate your model on test data.

And Show some results
"""

results = model.evaluate(x_test_flatten, y_test)
print("Loss = ",results[0])
print("Accuracy = ",results[1]*100,"%")

# Plot Accuracy...
plt.plot(history.history['accuracy'], label="Training accuracy")
plt.plot(history.history['val_accuracy'], label="validation Accuracy")
plt.title("Model accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Similarly write code to plot loss...
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss versus Epochs')
plt.legend()
plt.show()

"""Lets show our results on images from testing dataset"""

n = 5   # = no. of images to see predictions on

index = np.random.choice(len(x_test), n, replace=False)  # choose random index from test data
print("label: ")

for i,ind in enumerate(index):
    plt.subplot(1,n,i+1)
    plt.imshow(x_test[ind].reshape(28,28), cmap='gray')             # fill code to show images from test set
    plt.axis("off")
    print(y_test[ind],end="       ")

plt.show()
print("Predicted value: ")

# Now lets print the predictions

for i,ind in enumerate(index):
    # write code to predict and print digit in image
    prediciton = model.predeict(x_test[ind].reshape(1,784))
    # Hint: the output of the model is a 10-d vector which gives probabilties
    # The digit in the image would be the class for which probability is hghest...

    digit = np.argmax(prediction)
    print(digit,end="      ")